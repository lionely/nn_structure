{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "QIfQ6vcSsCFU"
   },
   "outputs": [],
   "source": [
    "# import pytorch libraries\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import torch \n",
    "import torch.autograd as autograd \n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import time\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "l0cm2wFgsCFZ"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from ripser import Rips\n",
    "import persim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(device(type='cuda'), True)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device,torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LVxeX6LCsCFe"
   },
   "source": [
    "# Loading MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "xAlFzQclsCFf"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2686,
     "status": "ok",
     "timestamp": 1532073682927,
     "user": {
      "displayName": "MSRI DeepLearn",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "105243882236575703967"
     },
     "user_tz": 420
    },
    "id": "pRgyL85osCFh",
    "outputId": "b1885f52-e1c1-4b46-dfe6-b602e2c34092"
   },
   "outputs": [],
   "source": [
    "transform=transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),transforms.Normalize((0.1307,), (0.3081,)) ]) #,transforms.Normalize((0.1307,), (0.3081,)) using to Normalize doesn't help accuracy it seems.\n",
    "\n",
    "# Load and transform data\n",
    "trainset = torchvision.datasets.MNIST('/tmp', train=True, download=False, transform=transform)\n",
    "testset = torchvision.datasets.MNIST('/tmp', train=False, download=False, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "6avkhDtssCFk"
   },
   "outputs": [],
   "source": [
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=BATCH_SIZE, shuffle=True, num_workers=32)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=BATCH_SIZE, shuffle=False, num_workers=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "k8JvqklasCFm"
   },
   "outputs": [],
   "source": [
    "def train_model(train_loader,num_epochs,model,optimizer):\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()# model is ready to have weights updated\n",
    "        for i, (images,labels ) in enumerate(train_loader):\n",
    "            #print(images.shape[2]*images.shape[3]) \n",
    "            images = images.view(-1,images.shape[2]*images.shape[3]) # making into a column vector\n",
    "            images = Variable(images, requires_grad=False).to(device)#.cuda()\n",
    "            \n",
    "            labels = labels.type(torch.LongTensor)\n",
    "            labels = Variable(labels, requires_grad=False).to(device)#.cuda()\n",
    "            # Forward + Backaward + Optimization\n",
    "            optimizer.zero_grad()\n",
    "            y_hat = model(images)\n",
    "            criterion = nn.CrossEntropyLoss()\n",
    "            loss = criterion(y_hat,labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "YBb84YOMsCFp"
   },
   "outputs": [],
   "source": [
    "def model_accuracy_loss(model, dataloader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    sum_loss = 0.0\n",
    "    total = 0\n",
    "    for images, labels in dataloader:\n",
    "        images = Variable(images.view(-1, 28*28)).to(device)#.cuda()\n",
    "        labels = Variable(labels).to(device)#.cuda()\n",
    "       \n",
    "        outputs = model(images)\n",
    "        _, pred = torch.max(outputs.data, 1)# gives maxes and indices locations\n",
    "        loss = F.cross_entropy(outputs, labels)\n",
    "        sum_loss += labels.size(0)*loss.item()\n",
    "        total += labels.size(0)\n",
    "        correct += pred.eq(labels.data).cpu().sum() \n",
    "    return 100 *correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "FMtTzejdsCF1"
   },
   "outputs": [],
   "source": [
    "def train_and_eval(model,num_epochs,optimizer,dataloader,isTrain=True):\n",
    "    \"\"\"\n",
    "    This function trains and then immediately evaluates the model \n",
    "    on the MNIST database.\n",
    "    \"\"\"\n",
    "    if isTrain:\n",
    "        train_model(trainloader,num_epochs,model,optimizer)\n",
    "    accuracy = model_accuracy_loss(model,dataloader)\n",
    "    \n",
    "    return float(accuracy.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "g5eBcWd3sCF4"
   },
   "outputs": [],
   "source": [
    "def run_experiment(activation_fns, num_epochs, num_runs, activation_acc_results,activation_names ,dataloader,isTrain=True):\n",
    "    # Our constant structure :\n",
    "    # Input: 784 Neurons\n",
    "    # H1 : 256\n",
    "    # H2 : 128\n",
    "    # Output : 10\n",
    "    for run in range(num_runs):\n",
    "        if run%5==0:\n",
    "              print(\"Run: \"+str(run)+\" Epoch: \"+str(num_epochs))\n",
    "        for i in range(len(activation_names)):\n",
    "            act_name = activation_names[i]\n",
    "            experiment_model = create_swish_model(256,128,activation_fns[act_name])\n",
    "            if isTrain:\n",
    "                experiment_model.load_state_dict(torch.load('experiment weights/experiment_model_'+str(run)))\n",
    "            else:\n",
    "                experiment_model.load_state_dict(torch.load('trained weights/'+act_name+'experiment_model_'+str(run)+str(num_epochs)))\n",
    "            if use_cuda and torch.cuda.is_available():\n",
    "                experiment_model.cuda()\n",
    "                \n",
    "            #Hyper Params constant for this experiment\n",
    "            learning_rate = 0.01\n",
    "            optimizer = torch.optim.Adam(experiment_model.parameters(), lr=learning_rate)\n",
    "            acc = train_and_eval(experiment_model,num_epochs,optimizer,dataloader,isTrain)\n",
    "            torch.save(experiment_model.state_dict(), 'trained weights/'+act_name+'experiment_model_'+str(run)+str(num_epochs))\n",
    "            activation_acc_results[act_name].append(acc)\n",
    "    return activation_acc_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qUpZsllxsCGe"
   },
   "source": [
    "# Creating a Distance Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(H1,H2,activation):\n",
    "    model = nn.Sequential(nn.Linear(28*28,H1,bias=False),\n",
    "                               activation(),\n",
    "                              nn.Linear(H1,H2,bias=False),\n",
    "                              activation(),\n",
    "                               nn.Linear(H2,10,bias=False))\n",
    "    return model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check if matrix is symmetric\n",
    "def check_symmetric(a, rtol=1e-05, atol=1e-08):\n",
    "    return np.allclose(a, a.T, rtol=rtol, atol=atol)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Steps:\n",
    "    * Create a model\n",
    "    * Get that model's weights\n",
    "    * for each node in the network, get the weights associated with it. (We are ignoring the biases atm)\n",
    "    * put (1-weights) into the weight matrix\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to create initial distance matrix for a vanilla neural network\n",
    "def make_dist_mat(model,output_dims):\n",
    "    dims = 0\n",
    "    for name,param in model.named_parameters():\n",
    "        dims+=param.cpu().detach().numpy().shape[1]\n",
    "    dims+= output_dims\n",
    "    distance_mat = np.zeros((dims,dims))\n",
    "    distance_mat += 2.# setting large distance\n",
    "    # setting diagonal to 0\n",
    "    for i in range(len(distance_mat)):\n",
    "        distance_mat[i,i] = 0.\n",
    "    return distance_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_parameters(model):\n",
    "    # we do 1-weights ,since some weights are negative. We do this to keep all weights within the range \n",
    "    # 1~2.\n",
    "    parameters = [] \n",
    "    for name,param in model.named_parameters():\n",
    "        parameters.append(1-param.cpu().detach().numpy())\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to build distance matrix based on model parameters.\n",
    "def build_dist_mat(dist_mat, params, inp_dim, sec_dim, third_dim, out_dim):\n",
    "    first_zero_blocks = inp_dim\n",
    "    \n",
    "    #1st layer by column\n",
    "    layer_1_dim = params[0].shape#(256,784)\n",
    "    # (num neurons current layer, cur num neurons+num neurons next layer)\n",
    "    dist_mat[first_zero_blocks:first_zero_blocks+layer_1_dim[0], \n",
    "                 :layer_1_dim[1]] = params[0]\n",
    "    \n",
    "    # 2nd layer by column\n",
    "    second_zero_blocks = sec_dim#(256,256)\n",
    "    layer_2_dim = parameters[1].shape\n",
    "    dist_mat[:first_zero_blocks, \n",
    "                  first_zero_blocks:layer_1_dim[0]+first_zero_blocks] = params[0].T\n",
    "\n",
    "    dist_mat[first_zero_blocks+layer_1_dim[0]:first_zero_blocks+layer_1_dim[0]+layer_2_dim[0]\n",
    "             ,first_zero_blocks:layer_1_dim[0]+first_zero_blocks] = params[1]\n",
    "    \n",
    "    # 3rd layer by column\n",
    "    third_zero_blocks = third_dim #(128,128) these zero matricies should be square\n",
    "    layer_3_dim = parameters[2].shape\n",
    "    dist_mat[first_zero_blocks:first_zero_blocks+layer_2_dim[1], \n",
    "                 first_zero_blocks+second_zero_blocks:first_zero_blocks\n",
    "                 +second_zero_blocks+third_zero_blocks] = params[1].T\n",
    "\n",
    "    dist_mat[first_zero_blocks+second_zero_blocks+third_zero_blocks:,\n",
    "                first_zero_blocks+second_zero_blocks:\n",
    "                 first_zero_blocks+second_zero_blocks+third_zero_blocks] = params[2]\n",
    "    # 4th layer by column\n",
    "    fourth_zero_blocks = out_dim\n",
    "    dist_mat[first_zero_blocks+second_zero_blocks:first_zero_blocks\n",
    "                 +second_zero_blocks+third_zero_blocks,\n",
    "                 first_zero_blocks\n",
    "                 +second_zero_blocks+third_zero_blocks:] = params[2].T\n",
    "    return dist_mat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Non trained Persistence Diagram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = Rips()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "not_train_diagram = r.fit_transform(distance_mat,distance_matrix=True)\n",
    "end = time.time()\n",
    "print(\"Elapsed: \"+str((end-start)/60.)+\" minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r.plot([pd_load['pd'][0],pd_load['pd'][1]], show=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving Distance Matricies and PD intervals to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # remember to plot pd need to be [pd[0],pd[1]]\n",
    "# def compress_save(data,is_mat=True,train=True,name):\n",
    "#     \"\"\"\n",
    "#     :param data is a distance matrix or persisten diagram intervals\n",
    "#     :param is_mat a boolean which says if the input is a matrix or not\n",
    "#     :param train a boolean indicating if the network was trained, this decies where we save these arrays\n",
    "#     :param name a string for the file name\n",
    "#     \"\"\"\n",
    "#     if train:\n",
    "#         np.savez('experiment_data/distance_matricies/trained/dist_mat-'+name, dist_mat=data)\n",
    "#     if is_mat and not train:\n",
    "#         np.savez('experiment_data/distance_matricies/not-trained/dist_mat-'+name, dist_mat=data)\n",
    "#     if not is_mat and train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rips(maxdim=0, thresh=inf, coeff=2, do_cocycles=False, n_perm = None, verbose=True)\n"
     ]
    }
   ],
   "source": [
    "num_models = 50\n",
    "num_epochs = 5#n-1\n",
    "first_layer_dim = 256#10\n",
    "second_layer_dim = 128#8\n",
    "model_performances = {}\n",
    "r = Rips(maxdim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed Experiment Time: 32.459229334195456 minutes\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "for i in range(num_models):\n",
    "    name = 'model_'+str(i)+'_ne-'+str(0)\n",
    "    PATH = 'experiment_data/model_params/'+'model_'+str(i)\n",
    "    model = create_model(first_layer_dim,second_layer_dim,nn.ReLU)\n",
    "    torch.save(model.state_dict(), PATH)\n",
    "    \n",
    "    distance_mat = make_dist_mat(model,10)# 10 = out_dim\n",
    "    parameters = weight_parameters(model)\n",
    "    distance_mat = build_dist_mat(distance_mat,parameters,784,first_layer_dim,second_layer_dim,10)\n",
    "    \n",
    "    #accuracy before training\n",
    "    model_performances[name] = [model_accuracy_loss(model,testloader).item()]\n",
    "    #save dist_mat before training\n",
    "    np.savez('experiment_data/distance_matricies/not-trained/dist_mat-'+name, dist_mat=distance_mat)\n",
    "    \n",
    "    #saving pd diagram\n",
    "    if check_symmetric(distance_mat): \n",
    "        not_train_diagram = r.fit_transform(distance_mat,distance_matrix=True)\n",
    "        np.savez('experiment_data/pd_intervals/not-trained/pd-'+name, pd=not_train_diagram)\n",
    "    \n",
    "    for e in range(num_epochs):\n",
    "        model.load_state_dict(torch.load(PATH))\n",
    "        name = 'model_'+str(i)+'_ne-'+str(e+1) # name after training\n",
    "        # now train the model\n",
    "        optimizer = optim.Adam(model.parameters())\n",
    "        train_and_eval(model,e+1,optimizer,trainloader)\n",
    "        #performance after training\n",
    "        model_performances[name] = [model_accuracy_loss(model,testloader).item()]\n",
    "        \n",
    "        #new dist mat after training\n",
    "        parameters = weight_parameters(model)\n",
    "        distance_mat = build_dist_mat(distance_mat,parameters,784,first_layer_dim,second_layer_dim,10)\n",
    "        #save dist_mat after training\n",
    "        np.savez('experiment_data/distance_matricies/trained/dist_mat-'+name, dist_mat=distance_mat)\n",
    "        #saving pd diagram\n",
    "        if check_symmetric(distance_mat): \n",
    "            train_diagram = r.fit_transform(distance_mat,distance_matrix=True)\n",
    "            np.savez('experiment_data/pd_intervals/trained/pd-'+name, pd=train_diagram)\n",
    "end = time.time()\n",
    "print(\"Elapsed Experiment Time: \"+str((end-start)/60.)+\" minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Should also record accuracy for each network before and after training and \n",
    "#how many epochs a model was training for\n",
    "pd.DataFrame(data=model_performances,index=['Accuracy']).to_csv('experiment_data/model_accuracies.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model_0_ne-0': [10],\n",
       " 'model_0_ne-1': [93],\n",
       " 'model_0_ne-2': [95],\n",
       " 'model_0_ne-3': [96],\n",
       " 'model_0_ne-4': [97],\n",
       " 'model_0_ne-5': [97],\n",
       " 'model_1_ne-0': [9],\n",
       " 'model_1_ne-1': [94],\n",
       " 'model_1_ne-2': [96],\n",
       " 'model_1_ne-3': [96],\n",
       " 'model_1_ne-4': [97],\n",
       " 'model_1_ne-5': [97],\n",
       " 'model_2_ne-0': [11],\n",
       " 'model_2_ne-1': [93],\n",
       " 'model_2_ne-2': [95],\n",
       " 'model_2_ne-3': [96],\n",
       " 'model_2_ne-4': [97],\n",
       " 'model_2_ne-5': [97],\n",
       " 'model_3_ne-0': [14],\n",
       " 'model_3_ne-1': [93],\n",
       " 'model_3_ne-2': [95],\n",
       " 'model_3_ne-3': [96],\n",
       " 'model_3_ne-4': [96],\n",
       " 'model_3_ne-5': [97],\n",
       " 'model_4_ne-0': [13],\n",
       " 'model_4_ne-1': [94],\n",
       " 'model_4_ne-2': [95],\n",
       " 'model_4_ne-3': [96],\n",
       " 'model_4_ne-4': [96],\n",
       " 'model_4_ne-5': [97],\n",
       " 'model_5_ne-0': [5],\n",
       " 'model_5_ne-1': [93],\n",
       " 'model_5_ne-2': [95],\n",
       " 'model_5_ne-3': [96],\n",
       " 'model_5_ne-4': [97],\n",
       " 'model_5_ne-5': [97],\n",
       " 'model_6_ne-0': [10],\n",
       " 'model_6_ne-1': [94],\n",
       " 'model_6_ne-2': [96],\n",
       " 'model_6_ne-3': [96],\n",
       " 'model_6_ne-4': [97],\n",
       " 'model_6_ne-5': [97],\n",
       " 'model_7_ne-0': [6],\n",
       " 'model_7_ne-1': [94],\n",
       " 'model_7_ne-2': [95],\n",
       " 'model_7_ne-3': [96],\n",
       " 'model_7_ne-4': [97],\n",
       " 'model_7_ne-5': [97],\n",
       " 'model_8_ne-0': [6],\n",
       " 'model_8_ne-1': [93],\n",
       " 'model_8_ne-2': [96],\n",
       " 'model_8_ne-3': [96],\n",
       " 'model_8_ne-4': [97],\n",
       " 'model_8_ne-5': [97],\n",
       " 'model_9_ne-0': [9],\n",
       " 'model_9_ne-1': [94],\n",
       " 'model_9_ne-2': [95],\n",
       " 'model_9_ne-3': [96],\n",
       " 'model_9_ne-4': [96],\n",
       " 'model_9_ne-5': [97],\n",
       " 'model_10_ne-0': [9],\n",
       " 'model_10_ne-1': [93],\n",
       " 'model_10_ne-2': [95],\n",
       " 'model_10_ne-3': [96],\n",
       " 'model_10_ne-4': [97],\n",
       " 'model_10_ne-5': [97],\n",
       " 'model_11_ne-0': [11],\n",
       " 'model_11_ne-1': [93],\n",
       " 'model_11_ne-2': [95],\n",
       " 'model_11_ne-3': [96],\n",
       " 'model_11_ne-4': [97],\n",
       " 'model_11_ne-5': [97],\n",
       " 'model_12_ne-0': [10],\n",
       " 'model_12_ne-1': [93],\n",
       " 'model_12_ne-2': [96],\n",
       " 'model_12_ne-3': [96],\n",
       " 'model_12_ne-4': [97],\n",
       " 'model_12_ne-5': [97],\n",
       " 'model_13_ne-0': [11],\n",
       " 'model_13_ne-1': [94],\n",
       " 'model_13_ne-2': [96],\n",
       " 'model_13_ne-3': [97],\n",
       " 'model_13_ne-4': [97],\n",
       " 'model_13_ne-5': [97],\n",
       " 'model_14_ne-0': [15],\n",
       " 'model_14_ne-1': [93],\n",
       " 'model_14_ne-2': [95],\n",
       " 'model_14_ne-3': [96],\n",
       " 'model_14_ne-4': [97],\n",
       " 'model_14_ne-5': [97],\n",
       " 'model_15_ne-0': [6],\n",
       " 'model_15_ne-1': [94],\n",
       " 'model_15_ne-2': [96],\n",
       " 'model_15_ne-3': [96],\n",
       " 'model_15_ne-4': [97],\n",
       " 'model_15_ne-5': [97],\n",
       " 'model_16_ne-0': [7],\n",
       " 'model_16_ne-1': [93],\n",
       " 'model_16_ne-2': [96],\n",
       " 'model_16_ne-3': [96],\n",
       " 'model_16_ne-4': [97],\n",
       " 'model_16_ne-5': [97],\n",
       " 'model_17_ne-0': [10],\n",
       " 'model_17_ne-1': [93],\n",
       " 'model_17_ne-2': [96],\n",
       " 'model_17_ne-3': [96],\n",
       " 'model_17_ne-4': [97],\n",
       " 'model_17_ne-5': [97],\n",
       " 'model_18_ne-0': [11],\n",
       " 'model_18_ne-1': [93],\n",
       " 'model_18_ne-2': [94],\n",
       " 'model_18_ne-3': [96],\n",
       " 'model_18_ne-4': [97],\n",
       " 'model_18_ne-5': [97],\n",
       " 'model_19_ne-0': [8],\n",
       " 'model_19_ne-1': [93],\n",
       " 'model_19_ne-2': [96],\n",
       " 'model_19_ne-3': [96],\n",
       " 'model_19_ne-4': [97],\n",
       " 'model_19_ne-5': [97],\n",
       " 'model_20_ne-0': [10],\n",
       " 'model_20_ne-1': [94],\n",
       " 'model_20_ne-2': [96],\n",
       " 'model_20_ne-3': [96],\n",
       " 'model_20_ne-4': [97],\n",
       " 'model_20_ne-5': [97],\n",
       " 'model_21_ne-0': [11],\n",
       " 'model_21_ne-1': [94],\n",
       " 'model_21_ne-2': [95],\n",
       " 'model_21_ne-3': [96],\n",
       " 'model_21_ne-4': [97],\n",
       " 'model_21_ne-5': [97],\n",
       " 'model_22_ne-0': [10],\n",
       " 'model_22_ne-1': [93],\n",
       " 'model_22_ne-2': [95],\n",
       " 'model_22_ne-3': [96],\n",
       " 'model_22_ne-4': [97],\n",
       " 'model_22_ne-5': [97],\n",
       " 'model_23_ne-0': [8],\n",
       " 'model_23_ne-1': [94],\n",
       " 'model_23_ne-2': [96],\n",
       " 'model_23_ne-3': [96],\n",
       " 'model_23_ne-4': [97],\n",
       " 'model_23_ne-5': [97],\n",
       " 'model_24_ne-0': [10],\n",
       " 'model_24_ne-1': [93],\n",
       " 'model_24_ne-2': [95],\n",
       " 'model_24_ne-3': [96],\n",
       " 'model_24_ne-4': [97],\n",
       " 'model_24_ne-5': [97],\n",
       " 'model_25_ne-0': [9],\n",
       " 'model_25_ne-1': [93],\n",
       " 'model_25_ne-2': [95],\n",
       " 'model_25_ne-3': [96],\n",
       " 'model_25_ne-4': [97],\n",
       " 'model_25_ne-5': [97],\n",
       " 'model_26_ne-0': [10],\n",
       " 'model_26_ne-1': [93],\n",
       " 'model_26_ne-2': [95],\n",
       " 'model_26_ne-3': [96],\n",
       " 'model_26_ne-4': [97],\n",
       " 'model_26_ne-5': [97],\n",
       " 'model_27_ne-0': [8],\n",
       " 'model_27_ne-1': [94],\n",
       " 'model_27_ne-2': [96],\n",
       " 'model_27_ne-3': [96],\n",
       " 'model_27_ne-4': [97],\n",
       " 'model_27_ne-5': [97],\n",
       " 'model_28_ne-0': [12],\n",
       " 'model_28_ne-1': [93],\n",
       " 'model_28_ne-2': [96],\n",
       " 'model_28_ne-3': [96],\n",
       " 'model_28_ne-4': [97],\n",
       " 'model_28_ne-5': [97],\n",
       " 'model_29_ne-0': [11],\n",
       " 'model_29_ne-1': [93],\n",
       " 'model_29_ne-2': [95],\n",
       " 'model_29_ne-3': [96],\n",
       " 'model_29_ne-4': [97],\n",
       " 'model_29_ne-5': [97],\n",
       " 'model_30_ne-0': [12],\n",
       " 'model_30_ne-1': [94],\n",
       " 'model_30_ne-2': [95],\n",
       " 'model_30_ne-3': [96],\n",
       " 'model_30_ne-4': [97],\n",
       " 'model_30_ne-5': [97],\n",
       " 'model_31_ne-0': [9],\n",
       " 'model_31_ne-1': [94],\n",
       " 'model_31_ne-2': [96],\n",
       " 'model_31_ne-3': [96],\n",
       " 'model_31_ne-4': [97],\n",
       " 'model_31_ne-5': [97],\n",
       " 'model_32_ne-0': [9],\n",
       " 'model_32_ne-1': [93],\n",
       " 'model_32_ne-2': [95],\n",
       " 'model_32_ne-3': [96],\n",
       " 'model_32_ne-4': [97],\n",
       " 'model_32_ne-5': [97],\n",
       " 'model_33_ne-0': [10],\n",
       " 'model_33_ne-1': [94],\n",
       " 'model_33_ne-2': [95],\n",
       " 'model_33_ne-3': [96],\n",
       " 'model_33_ne-4': [97],\n",
       " 'model_33_ne-5': [97],\n",
       " 'model_34_ne-0': [10],\n",
       " 'model_34_ne-1': [94],\n",
       " 'model_34_ne-2': [95],\n",
       " 'model_34_ne-3': [96],\n",
       " 'model_34_ne-4': [97],\n",
       " 'model_34_ne-5': [97],\n",
       " 'model_35_ne-0': [8],\n",
       " 'model_35_ne-1': [94],\n",
       " 'model_35_ne-2': [96],\n",
       " 'model_35_ne-3': [96],\n",
       " 'model_35_ne-4': [97],\n",
       " 'model_35_ne-5': [97],\n",
       " 'model_36_ne-0': [8],\n",
       " 'model_36_ne-1': [93],\n",
       " 'model_36_ne-2': [96],\n",
       " 'model_36_ne-3': [96],\n",
       " 'model_36_ne-4': [97],\n",
       " 'model_36_ne-5': [97],\n",
       " 'model_37_ne-0': [15],\n",
       " 'model_37_ne-1': [93],\n",
       " 'model_37_ne-2': [95],\n",
       " 'model_37_ne-3': [96],\n",
       " 'model_37_ne-4': [97],\n",
       " 'model_37_ne-5': [97],\n",
       " 'model_38_ne-0': [10],\n",
       " 'model_38_ne-1': [94],\n",
       " 'model_38_ne-2': [96],\n",
       " 'model_38_ne-3': [96],\n",
       " 'model_38_ne-4': [97],\n",
       " 'model_38_ne-5': [97],\n",
       " 'model_39_ne-0': [9],\n",
       " 'model_39_ne-1': [94],\n",
       " 'model_39_ne-2': [96],\n",
       " 'model_39_ne-3': [96],\n",
       " 'model_39_ne-4': [97],\n",
       " 'model_39_ne-5': [97],\n",
       " 'model_40_ne-0': [15],\n",
       " 'model_40_ne-1': [93],\n",
       " 'model_40_ne-2': [95],\n",
       " 'model_40_ne-3': [96],\n",
       " 'model_40_ne-4': [97],\n",
       " 'model_40_ne-5': [97],\n",
       " 'model_41_ne-0': [10],\n",
       " 'model_41_ne-1': [93],\n",
       " 'model_41_ne-2': [95],\n",
       " 'model_41_ne-3': [96],\n",
       " 'model_41_ne-4': [97],\n",
       " 'model_41_ne-5': [97],\n",
       " 'model_42_ne-0': [8],\n",
       " 'model_42_ne-1': [94],\n",
       " 'model_42_ne-2': [95],\n",
       " 'model_42_ne-3': [96],\n",
       " 'model_42_ne-4': [97],\n",
       " 'model_42_ne-5': [97],\n",
       " 'model_43_ne-0': [9],\n",
       " 'model_43_ne-1': [93],\n",
       " 'model_43_ne-2': [95],\n",
       " 'model_43_ne-3': [96],\n",
       " 'model_43_ne-4': [96],\n",
       " 'model_43_ne-5': [97],\n",
       " 'model_44_ne-0': [8],\n",
       " 'model_44_ne-1': [93],\n",
       " 'model_44_ne-2': [95],\n",
       " 'model_44_ne-3': [96],\n",
       " 'model_44_ne-4': [97],\n",
       " 'model_44_ne-5': [97],\n",
       " 'model_45_ne-0': [8],\n",
       " 'model_45_ne-1': [94],\n",
       " 'model_45_ne-2': [96],\n",
       " 'model_45_ne-3': [96],\n",
       " 'model_45_ne-4': [97],\n",
       " 'model_45_ne-5': [97],\n",
       " 'model_46_ne-0': [7],\n",
       " 'model_46_ne-1': [94],\n",
       " 'model_46_ne-2': [96],\n",
       " 'model_46_ne-3': [96],\n",
       " 'model_46_ne-4': [97],\n",
       " 'model_46_ne-5': [97],\n",
       " 'model_47_ne-0': [6],\n",
       " 'model_47_ne-1': [93],\n",
       " 'model_47_ne-2': [95],\n",
       " 'model_47_ne-3': [96],\n",
       " 'model_47_ne-4': [96],\n",
       " 'model_47_ne-5': [97],\n",
       " 'model_48_ne-0': [13],\n",
       " 'model_48_ne-1': [93],\n",
       " 'model_48_ne-2': [95],\n",
       " 'model_48_ne-3': [96],\n",
       " 'model_48_ne-4': [97],\n",
       " 'model_48_ne-5': [97],\n",
       " 'model_49_ne-0': [10],\n",
       " 'model_49_ne-1': [93],\n",
       " 'model_49_ne-2': [96],\n",
       " 'model_49_ne-3': [96],\n",
       " 'model_49_ne-4': [97],\n",
       " 'model_49_ne-5': [97]}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez('experiment_data/distance_matricies/not-trained/dist_mat',lol=distance_mat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_mat = np.load('experiment_data/distance_matricies/not-trained/dist_mat.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_mat['lol']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez('experiment_data/distance_matricies/not-trained/not_train_pd',pd=not_train_diagram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_load = np.load('experiment_data/distance_matricies/not-trained/not_train_pd.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_load['pd'][]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train a model then persistence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_model = create_model(first_layer_dim,second_layer_dim,nn.ReLU)\n",
    "optimizer = optim.Adam(trained_model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#before train\n",
    "model_accuracy_loss(trained_model,testloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_and_eval(trained_model,0,optimizer,trainloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_accuracy_loss(trained_model,testloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_distance_mat = make_dist_mat(trained_model,10)\n",
    "parameters = weight_parameters(trained_model)\n",
    "train_distance_mat = build_dist_mat(train_distance_mat,parameters,784,first_layer_dim,second_layer_dim,10)\n",
    "train_distance_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_distance_mat[785]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.imshow(train_distance_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_symmetric(train_distance_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "train_pd = r.fit_transform(train_distance_mat, distance_matrix=True)\n",
    "end = time.time()\n",
    "print(\"Elapsed: \"+str((end-start)/60.)+\" minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r.plot(train_pd, show=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "print(persim.bottleneck(train_pd[0], not_train_diagram[0])) #H_0\n",
    "end = time.time()\n",
    "print(\"Elapsed: \"+str((end-start)/60.)+\" minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pd[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#H_1 gives a memory error too many simplicies around 230k\n",
    "start = time.time()\n",
    "print(persim.bottleneck(train_pd[1][:1000], not_train_diagram[1][:1000])) #H_1 distance\n",
    "end = time.time()\n",
    "print(\"Elapsed: \"+str((end-start)/60.)+\" minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path homology for direct graph homology\n",
    "# directed clique homology no software avail\n",
    "# can look at homology fluctates per epoch\n",
    "# find bottleneck distance"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "default_view": {},
   "name": "MNIST MLP, Swish Experiments NO NOISE.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
